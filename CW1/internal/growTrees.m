function tree = growtrees(data,param)

%        Base               Each node stores:
%         1                   trees.idx       - data (index only) which split into this node
%        / \                  trees.t         - threshold of split function
%       2   3                 trees.dim       - feature dimension of split function
%      / \ / \                trees.prob      - class distribution of this node
%     4  5 6  7               trees.leaf_idx  - leaf node index (empty if it is not a leaf node) 

disp('Training Random Forest...');

[N,D] = size(data);

% Bootstrap sampling fraction: 1 - 1/e (63.2%)
frac = 1 - 1/exp(1);

cnt_total = 1;

% get all the unique labels used to classify data.
[labels,~] = unique(data(:,end));

%training the specified number of trees.
for T = 1:param.num
    
    % Bootstraping aggregating
    % A new training set for each tree is generated by random sampling from dataset WITH replacement.
    idx = randsample(N,ceil(N*frac),1); 
   
    % Histogram class distribution.
    % Information not currently used.
    prior = histc(data(idx,end),labels)/length(idx);
    
    % Initialise base node
    tree(T).node(1) = struct('idx',idx,'t',nan,'dim',-1,'prob',[]);
    
    % Split Nodes - [2^(param.depth-1)-1] all nodes bar leaves
    for n = 1:2^(param.depth-1)-1
        % split node will output the root node, the left and right notes
        [tree(T).node(n),tree(T).node(n*2),tree(T).node(n*2+1)] = splitNode(data,tree(T).node(n),param);
    end
    
    % Leaf Nodes
    cnt = 1;
    
    % Go through all the nodes and leaves
    for n = 1:2^param.depth-1
        
        % skip the node if no points were passed to it
        % i.e. this node does not actually exist in the tree
        % this is NOT checking if the node is a leaft
        % to check if node is a left, we will instead check if it has a
        % split dimension
        if ~isempty(tree(T).node(n).idx)
            
            % Percentage of observations of each class label
            tree(T).node(n).prob = histc(data(tree(T).node(n).idx,end),labels)/length(tree(T).node(n).idx);
            
            % if this is a leaf node
            % leaves will not have split functions and thus, no split dimension
            if ~tree(T).node(n).dim

                % this line will create a new field called leaf_idx within the node structure of tree
                tree(T).node(n).leaf_idx = cnt;
                % this will leave a new field called leaf in within the tree structure 
                tree(T).leaf(cnt).label = cnt_total;
                % creates the probability distribution of classes for that node
                prob = reshape(histc(data(tree(T).node(n).idx,end),labels),[],1);
                % attaches the prob to the leaf structure within the tree structure
                tree(T).leaf(cnt).prob = prob; %.*prior; % Multiply by the prior probability of bootstrapped sub-training-set
                % Normalisation
                tree(T).leaf(cnt).prob = tree(T).leaf(cnt).prob./sum(tree(T).leaf(cnt).prob);
                
                % we attach the probabilies matrix only to the first tree rather than all the trees
                if strcmp(param.split,'Var')
                    tree(1).cc(cnt_total,:) = mean(data(tree(T).node(n).idx,1:end-1),1); % For RF clustering, unfinished
                else
                    tree(1).prob(cnt_total,:) = tree(T).node(n).prob';
                end
                
                cnt = cnt+1;
                cnt_total = cnt_total + 1;
            end
            
        end
        
    end
end
end